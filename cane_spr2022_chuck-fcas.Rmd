---
title: "Using Rating Factors to Learn Loss Distribution Parameters"
subtitle: "Including Some Applications<br/>⚔"
author: "Chuck Lindberg, FCAS"
date: "Spring BACE Meeting --- April 19, 2023"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  extra_dependencies:
    cancel: null
---

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(xaringanthemer)
library(kableExtra)
library(latex2exp)
library(actuar)
source("R/boost_colors.R")

set.seed(8888)

style_duo_accent(
  primary_color = "#16325f", 
  secondary_color = "#0097a7",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
  )

```

```{css, echo = FALSE}
.title-slide h1 {
  color: #e6e567;
}
.title-slide h2 {
  color: #c7c8ca;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
.tinier .remark-code { /*Change made here*/
  font-size: 40% !important;
}
.latex-red span.node-latex{
 color:red;
}
.pull-left-less {
  float: left;
  width: 54%;
}
.pull-right-more {
  float: right;
  width: 40%;
}
.pull-left-more {
  float: left;
  width: 40%;
}
.pull-right-less {
  float: right;
  width: 54%;
}
```

# Antitrust Notice

The Casualty Actuarial Society is committed to adhering strictly to the letter and spirit of the antitrust laws.  Seminars conducted under the auspices of the CAS are designed solely to provide a forum for the expression of various points of view on topics described in the programs or agendas for such meetings.  

Under no circumstances shall CAS seminars be used as a means for competing companies or firms to reach any understanding – expressed or implied – that restricts competition or in any way impairs the ability of members to exercise independent business judgment regarding matters affecting competition.  

It is the responsibility of all seminar participants to be aware of antitrust regulations, to prevent any written or verbal discussions that appear to violate these laws, and to adhere in every respect to the CAS antitrust compliance policy.

---

# First, a riddle

Draw one straight line to make the equation correct.

</br> 

$5 + 5 + 5 = 550$

--

</br> 

$5 + 5 + 5 \ne 550$

--

![](images/riddle.png)

.footnote[
There is more than one way to do what is demonstrated here today!
]

---

class: inverse, middle

# Content

- Concept of URF2LLDP
- Background Details
- Learning Process
- Applications
- Closing Remarks

---

# Concept of URF2LLDP

Suppose the following:

- We have a set of rating factors, particularly increased limit factors and/or deductible factors.  
- It is not known how the factors were originally created, but they are trusted and useful.*

**Can we find a loss distribution and learn it's parameters such that we can reasonably get close to the set of known rating factors?**

</br>

$$ILF(l) = \frac{E[X; l | \Theta]}{E[X; b | \Theta]} \text{  } \longleftarrow \text{what is } \Theta ?$$
.footnote[
[*] Examples: lost or outdated documentation, competitor filings, or maybe a loss distribution wasn't used at all to create the factors. 
]

---

class: inverse, center, middle

# Background Details

---

# Limited Average Severity

> The expected severity at a given limit of liability is known as the Limited Average Severity (LAS). Stated simply, the limited average severity is the average size of loss when all losses have been capped at the given policy limit. [1]

Limited average severity given policy limit $x$ is denoted as follows: $$LAS(x) = E[X; x] = \\ \int_{0}^{x} u\ dF(u)\ + x \cdot \big(1 - F(x) \big) $$

.pull-left[
Other literature may write this as follows:   $$LAS(x) = E[X \wedge x]$$
]

.pull-right[
```{r echo = FALSE, out.width = '70%'}
knitr::include_graphics("images/las_3m.png")
```
]

.footnote[
[1] [Palmer, 2006. Increased Limits <br/> Ratemaking for Liability Insurance](https://www.casact.org/sites/default/files/database/studynotes_palmer.pdf)
]

---

```{css, echo = FALSE}
header {
    margin: 0;
    padding: 0;
}
```

# Increased Limit Factors

.pull-left[

The ILF is the LAS at limit $l$ relative to the LAS at the base limit $b$.

$$ILF(l) = \frac{E[X; l]}{E[X; b]}$$

```{r echo = FALSE, out.width = '90%', dpi = 400}
knitr::include_graphics("images/ilf_3m.png")
```
]

.pull-right[

The deductible factor is calculated as a "decreased limit factor".

$$Ded(d) = ILF(d) = \frac{E[X; d]}{E[X; b]}$$

```{r echo = FALSE, out.width = '90%', dpi = 400}
knitr::include_graphics("images/ded_250k.png")
```
]

Rating often applies the factor: $\big( E[X; l] - E[X; d] \big) / E[X; b]$

---

# ILFs and Additional Provisions

It's possible the rating factors were derived including additional provisions.  If this is the case it may affect the learning process.

### ILF with Additional Provisions at Each Limit

$$ILF(l) = \frac{LAS(l) + ALAE(l) + ULAE(l) + RL(l)}{LAS(b) + ALAE(b) + ULAE(b) + RL(b)}$$

### Indemnity Only

$$ILF(l) = \frac{LAS(l)}{LAS(b)}$$

.footnote[
Additional provisions, such as ALAE, ULAE, and a Risk Load (RL), could be "flat"; i.e., they do not vary by limit.
]

---

# LAS and Loss Distributions

### Weibull (*β*, *δ*)

$$\small{\beta^m \cdot \Gamma \big((x / \beta)^{\delta}, 1+m/\delta \big) + x^m e^{-(x/\beta)^{\delta}}}$$

.tiny[
```{r eval = FALSE}
las_weibull <- function(x, beta, delta, m = 1){
  beta^m * inc_gamma((x/beta)^delta, 1 + m/delta) + x^m * exp(-(x/beta)^delta)
}
```
]

### Gamma (*α*, *β*)

$$\small{\beta^m \cdot \frac{\Gamma (x / \beta, \alpha + m)}{\Gamma(\alpha)} + x^m \bigg(1 - \frac{\Gamma(x / \beta, \alpha)}{\Gamma(\alpha)}\bigg)}$$
.tiny[
```{r eval = FALSE}
las_gamma <- function(x, alpha, beta m = 1){
  beta^m * inc_gamma(x/beta, alpha + m) / inc_gamma(Inf, α) + x^m * (1 - inc_gamma(x/beta, α) / inc_gamma(Inf, alpha))
}
```
]

---

# LAS and Loss Distributions

### Burr (*α*, *β*, *δ*)

$$\small{\frac{\beta^{m/\delta}}{\Gamma(\alpha)} \cdot \Gamma(\alpha - m/\delta) \cdot \Gamma(1 + m/\delta) \cdot B \Bigg(1+m/\delta, \alpha - m/\delta, 1 - \frac{1}{1+(x \ \beta)^{\delta}}\Bigg)  + \frac{x^m}{[1+(x \ \beta)^{\delta}]^\alpha}}$$
.tiny[
```{r eval = FALSE}
las_burr <- function(x, alpha, beta, delta, m = 1){
  :/ # I don't want to program this.
}
```
]

### Lognormal (*µ*, *σ*)

$$\small{e^{m\mu+m^2\sigma^2/2} \cdot \Phi\Bigg(\frac{\log x - \mu - m\sigma^2}{\sigma}\Bigg)+ x^m \cdot \Phi \Bigg(\frac{-\log x + \mu}{\sigma}\Bigg)}$$
.tiny[
```{r eval = FALSE}
las_lognormal <- function(x, mu, sigma, m = 1){
  :) # Not going to program this either.
}
```
]

---

# Incomplete Gamma Function

#### Clarification on Notation

Notation on the prior slides, namely, $\Gamma(x, \alpha)$ is meant to be the `lower incomplete gamma function`, i.e., $\gamma(x, \alpha)$. 

$$\small{\gamma(x, \alpha) = \int_{0}^{x} u^{\alpha - 1} e^{-u} du \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   \Gamma(x, \alpha) = \int_{x}^{\infty} u^{\alpha - 1} e^{-u} du}$$
$$\small{\Gamma(\alpha) = \int_{0}^{\infty} u^{\alpha - 1} e^{-u} du} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \gamma(x, \alpha)+\Gamma(x, \alpha)=\Gamma(\alpha)$$

**Actuarial texts use the notation $\Gamma(x, \alpha)$ to express integrating from $0$ to $x$.**

#### R Code

We can use the following function to "flip" the `gammainc` function provided by the `expint` package.

.tiny[
```{r echo = TRUE}
inc_gamma <- function(x, alpha, lower = TRUE){
  if(lower){ expint::gammainc(alpha, 0) - expint::gammainc(alpha, x) #Total area minus upper
  } else   { expint::gammainc(alpha, x) }
}
```
]

---

# Helper Functions

Turns out we don't need the functions on prior slides because we have the `actuar` package. This package is maintained and used by several other `R` packages and authors such as [Loss Data Analytics](https://openacttexts.github.io/Loss-Data-Analytics/index.html) [1].

```{r echo = FALSE}
tibble(
  `Chuck's Functions` = c("las_weibull",        "las_gamma",        "las_burr",        "las_lognormal"),
  `actuar Package`    = c("actuar::levweibull", "actuar::levgamma", "actuar::levburr", "actuar::levlnorm")
) %>% 
kbl("html") %>%
kable_styling(latex_options = "striped")
```

- The prefix `lev` stands for limited expected value in the `actuar` package.
- `actuar` utilizes the `expint` package like we did before.

.footnote[
[1] Loss Data Analytics, https://openacttexts.github.io/Loss-Data-Analytics/index.html, *An open text authored by the Actuarial Community*
]

---

class: inverse, center, middle

# Learning Process

---

# Learning

To demonstrate the learnings process we first start by generating ILFs using a Weibull distribution with parameters $\beta = 4000$ and $\delta = 0.25$.

.pull-left[
.tinier[
```{r}
beta  <- 4000
delta <- 0.25

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```
]
]

.pull-right[
```{r echo = FALSE, dpi = 400}
df %>% 
mutate(name = "Generated ILF") %>% 
ggplot(aes(x = limit, y = ilf)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost() +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) 
```
]

---

# Learning

To fit the ILFs (our target), we create a function that outputs the sum of squared errors (SSE) and use the `optim` function to solve for $\beta$ and $\delta$.

.pull-left[
.tinier[
```{r}
beta  <- 4000
delta <- 0.25

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```


```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(.,    shape = params[1], scale = params[2])/
    actuar::levweibull(10^6, shape = params[1], scale = params[2])
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```
]
]

.pull-right[
```{r echo = FALSE, dpi = 400}

# We repeat the code here to suppress warnings.
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(.,    shape = params[1], scale = params[2]) %>% suppressWarnings()/
    actuar::levweibull(10^6, shape = params[1], scale = params[2]) %>% suppressWarnings()
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

# Saving for later in presentation
delta_earlier <- delta
beta_earlier  <- beta

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )

df %>% 
pivot_longer(c(ilf, ilf_fitted)) %>% 
mutate(name = name %>% {if_else(. == "ilf", "Target", "ILF Fitted")}) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) + 
annotate("text", x = 2.5*10^6, y = 0.75, label = TeX(paste0("$\\widehat{\\beta} = ",  round(beta, 2), "$"),  output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 2.5*10^6, y = 0.65, label = TeX(paste0("$\\widehat{\\delta} = ", round(delta, 5), "$"), output = "character"), size = 5, parse = TRUE)
```
]

---

# Learning - where we started

The initial parameters we started with $(\beta = 2000$ and $\delta = 0.5)$ created a very different ILF curve than what was ultimately produced.

.pull-left[
.tinier[
```{r}
p <- c(delta = 0.5, beta = 2000) 

df <-
  df %>% 
  mutate(
    ilf_initial = 
      actuar::levweibull(limit, shape = p[1], scale = p[2])/
      actuar::levweibull(10^6,  shape = p[1], scale = p[2])
  )
```
]

- Picking initial parameters can be somewhat of a guessing game.

- Keep the relationship of the parameters and the distribution in mind when initializing.
]

.pull-right[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(ilf, ilf_fitted, ilf_initial)) %>% 
mutate(
  name = 
    name %>% 
    {if_else(. == "ilf", "Target", .)} %>% 
    str_replace_all("_", " ") %>% 
    str_to_title %>% 
    str_replace_all("Ilf", "ILF")
) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```
]

---

# Initial Parameters

With most learning processes the initial parameters play big role in the outcome. Below are four different examples.

```{r include = FALSE}
#Reset df for simplicity
beta  <- 4000
delta <- 0.25

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  ) 

p1   <- c(delta = 4, beta = 100000)
opt1 <- optim(par = p1, f) %>% suppressWarnings()

p2   <- c(delta = 0.15, beta = 8000)
opt2 <- optim(par = p2, f) %>% suppressWarnings()

p3   <- c(delta = 1, beta = 10000)
opt3 <- optim(par = p3, f) %>% suppressWarnings()

p4   <- c(delta = 2.5, beta = 60000)
opt4 <- optim(par = p4, f) %>% suppressWarnings()

df <-
  df %>% 
  mutate(
    ilf_1 = 
      actuar::levweibull(limit, shape = opt1$par[1], scale = opt1$par[2])/
      actuar::levweibull(10^6,  shape = opt1$par[1], scale = opt1$par[2]),
    ilf_2 = 
      actuar::levweibull(limit, shape = opt2$par[1], scale = opt2$par[2])/
      actuar::levweibull(10^6,  shape = opt2$par[1], scale = opt2$par[2]),
    ilf_3 = 
      actuar::levweibull(limit, shape = opt3$par[1], scale = opt3$par[2])/
      actuar::levweibull(10^6,  shape = opt3$par[1], scale = opt3$par[2]),
    ilf_4 = 
      actuar::levweibull(limit, shape = opt4$par[1], scale = opt4$par[2])/
      actuar::levweibull(10^6,  shape = opt4$par[1], scale = opt4$par[2])
  )
```

```{r echo = FALSE, dpi = 400, fig.width = 9, fig.height = 5}
df %>% 
pivot_longer(c(ilf_1, ilf_2, ilf_3, ilf_4)) %>% 
pivot_longer(c(ilf, value), names_to = "type") %>% 
mutate(
  name = 
    case_when(
      name == "ilf_1" ~ paste0("Initial Parameters = {",  p1[2] %>% format(big.mark = ",", scientific = FALSE), " | ", p1[1], "}"),
      name == "ilf_2" ~ paste0("Initial Parameters = {",  p2[2] %>% format(big.mark = ",", scientific = FALSE), " | ", p2[1], "}"),
      name == "ilf_3" ~ paste0("Initial Parameters = {",  p3[2] %>% format(big.mark = ",", scientific = FALSE), " | ", p3[1], "}"),
      name == "ilf_4" ~ paste0("Initial Parameters = {",  p4[2] %>% format(big.mark = ",", scientific = FALSE), " | ", p4[1], "}")
    ),
  type = type %>% {if_else(. == "ilf", "Target", "ILF Fitted")}
) %>% 
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = type), linewidth = 0.5) +
geom_point(aes(color = type), size = 2) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 10, hjust = 0.6),
  axis.text.y     = element_text(size = 10),
  axis.title.x    = element_text(size = 12),
  axis.title.y    = element_text(size = 12),
  legend.text     = element_text(size = 12),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) +
facet_wrap(~name, nrow = 2)
```

---

# Initial Parameters

.pull-left[
Ways to combat initial parameters:
- Trial and Error
- Inspect the curve implied
- Method of moments
- Add penalties...?

```{r include = FALSE}
#Reset df for simplicity
beta  <- 4000
delta <- 0.25

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  ) 

f <- function(params) { 
  
  fit  <- 
    tibble(
        fitted_ilf = 
          df$limit %>% 
          actuar::levweibull(.,    shape = params[1], scale = params[2]) %>% suppressWarnings/
          actuar::levweibull(10^6, shape = params[1], scale = params[2]) %>% suppressWarnings,
        penalty = (fitted_ilf == lag(fitted_ilf)) %>% replace_na(FALSE)
    ) 

  if(any(fit$penalty)){lambda <- 10^6} else {lambda <- 1}
  
  sum((df$ilf - fit$fitted_ilf)^2) * lambda # SSE x λ
}

p   <- c(delta = 0.15, beta = 8000)
opt <- optim(par = p, f)

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      limit %>% 
      actuar::levweibull(.,    shape = delta, scale = beta)/
      actuar::levweibull(10^6, shape = delta, scale = beta),
    ilf_initial =
      limit %>% 
      actuar::levweibull(.,    shape = p[1], scale = p[2])/
      actuar::levweibull(10^6, shape = p[1], scale = p[2])
  )
```

.tinier[

Example of adding a penalty when the curve is flat:
```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    tibble(
        fitted_ilf = 
          df$limit %>% 
          actuar::levweibull(.,    shape = params[1], scale = params[2])/
          actuar::levweibull(10^6, shape = params[1], scale = params[2]),
        penalty = (fitted_ilf == lag(fitted_ilf)) %>% replace_na(FALSE)
    ) 

  if(any(fit$penalty)){lambda <- 10^6} else {lambda <- 1}
  
  sum((df$ilf - fit$fitted_ilf)^2) * lambda # SSE x λ
}
```
]
]

.pull-right[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(ilf, ilf_fitted, ilf_initial)) %>% 
mutate(
  name = 
    name %>% 
    {if_else(. == "ilf", "Target", .)} %>% 
    str_replace_all("_", " ") %>% 
    str_to_title %>% 
    str_replace_all("Ilf", "ILF")
) %>% 
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```

]

More research is necessary to provide additional guidance on initial parameters. It might be better to create a customized learning algorithm rather than using `optim`. 

---

# Learning Additional Provisions

Suppose there is a suspected ALAE load that does not vary by limit. In other words, for any $l$ and $b$, $ALAE(l) = ALAE(b)$.

$$ILF(l) = \frac{LAS(l) + ALAE(l)}{LAS(b) + ALAE(b)}$$

.pull-left-more[
Testing showed that a fixed load parameter did not dramatically affect learning.  
]

.pull-right-less[
.tinier[
```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    (actuar::levweibull(df$limit, shape = params[1], scale = params[2]) + params[3]) /
    (actuar::levweibull(10^6,     shape = params[1], scale = params[2]) + params[3])
    
  sum((df$ilf - fit)^2) # SSE
  
}
```
]
]

<br> <br> <br> <br> <br> <br> **If you're unable to get a good fit using a distribution it does not imply the ILFs your trying to fit are improper.**  It's possible there was some other derivation or assumptions used such as:

- Risk load by limit
- Empirical methods
- Judgmental selection

---

# Fixed ALAE Load Parameter

Below is an example of fitting with an ALAE load that is fixed per limit.  

.pull-left-less[
.tinier[
```{r}
beta  <- 4000
delta <- 0.25
alae  <- 800

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = 
      (actuar::levweibull(limit, shape = delta, scale = beta) + alae)/
      (actuar::levweibull(10^6,  shape = delta, scale = beta) + alae),
    ilf_no_alae   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```


```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    (actuar::levweibull(df$limit, shape = params[1], scale = params[2]) + params[3]) /
    (actuar::levweibull(10^6,     shape = params[1], scale = params[2]) + params[3])
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000, alae = 500) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]
alae  <- opt$par[3]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      (actuar::levweibull(limit, shape = delta, scale = beta) + alae)/
      (actuar::levweibull(10^6,  shape = delta, scale = beta) + alae)
  )
```
]
]

.pull-right-more[
```{r echo = FALSE, dpi = 400}

# We repeat the code here to suppress warnings.
f <- function(params) { 
  
  fit  <- 
    (actuar::levweibull(df$limit, shape = params[1], scale = params[2]) + params[3]) %>% suppressWarnings()/
    (actuar::levweibull(10^6,     shape = params[1], scale = params[2]) + params[3]) %>% suppressWarnings()
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000, alae = 500) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]
alae  <- opt$par[3]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      (actuar::levweibull(limit, shape = delta, scale = beta) + alae)/
      (actuar::levweibull(10^6,  shape = delta, scale = beta) + alae)
  )

df %>% 
pivot_longer(c(ilf, ilf_fitted)) %>% 
mutate(name = name %>% {if_else(. == "ilf", "Target", "ILF Fitted")}) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) + 
annotate("text", x = 2.5*10^6, y = 0.75, label = TeX(paste0("$\\widehat{\\beta} = ",  round(beta, 2), "$"),  output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 2.5*10^6, y = 0.65, label = TeX(paste0("$\\widehat{\\delta} = ", round(delta, 5), "$"), output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 2.5*10^6, y = 0.55, label = TeX(paste0("$\\widehat{alae} = ",    round(alae, 5), "$"),  output = "character"), size = 5, parse = TRUE)
```
]

---

# Learning the Distribution

Learning the distribution is a bit tricky because you have to learn different sets of parameters and choose initial parameters for each.

```{r echo = FALSE, dpi = 400, fig.width = 9, fig.height = 5.4}
limits <- c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6

df <-
  tibble(
    distribution = 
      c("Weibull", "Gamma", "Burr", "Lognormal"),
    limit = list(limits),
    params = 
      list(
        list(limit = limits, shape  = 0.25, scale  = 4000),
        list(limit = limits, shape  = 0.1, scale  = 500000),
        list(limit = limits, shape1 = 0.3,  shape2 = 1),
        list(limit = limits, meanlog = log(300), sdlog = log(50))
        ),
    func = 
      c("levweibull", "levgamma", "levburr", "levlnorm")
  ) %>% 
  mutate(
    lev = invoke_map(func, params)
  )

df %>% 
unnest(c(limit, lev)) %>% 
ggplot(aes(x = limit, y = lev)) +
geom_line(aes(color  = distribution), linewidth = 0.5) +
geom_point(aes(color = distribution), size = 2) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  # legend.title    = element_blank(),
  axis.text.x     = element_text(size = 10, hjust = 0.6),
  axis.text.y     = element_text(size = 10),
  axis.title.x    = element_text(size = 12),
  axis.title.y    = element_text(size = 12),
  # legend.text     = element_text(size = 12),
  legend.position = "none"
) +
xlab("Limit") + 
ylab("Limited Average Severity") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
# scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
scale_y_continuous(labels = scales::number_format(big.mark = ",")) + 
theme(axis.text.x = element_text(angle = 10)) +
facet_wrap(~distribution, nrow = 2)
```

---

# Learning the Distribution

Learning the distribution can be achieved with an algorithm.

.pull-left[
**Step 1.** create a dataframe with all inputs:
  - Distribution
  - Initial parameters which coincide with the distribution
  - Limits

**Step 2.** create a function or set of functions to learn parameters.

**Step 3.** loop over the distributions or invoke map.

**Step 4.** choose distribution with the best fit.
]

.pull-right[

Example of `invoke_map` to produce limited average severity for each distribution with different parameters. This produced the graph on the last slide.

.tinier[
```{r eval = FALSE}
limits <- c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6

df <-
  tibble(
    distribution = 
      c("Weibull", "Gamma", "Burr", "Lognormal"),
    limit = list(limits),
    params = 
      list(
        list(limit = limits, shape  = 0.25, scale  = 4000),
        list(limit = limits, shape  = 0.1, scale  = 500000),
        list(limit = limits, shape1 = 0.3,  shape2 = 1),
        list(limit = limits, meanlog = log(300), sdlog = log(50))
        ),
    func = 
      c("levweibull", "levgamma", "levburr", "levlnorm")
  ) %>% 
  mutate(
    lev = invoke_map(func, params)
  )
```
]
]

---

# Scale ILFs to LAS

- The base limit has a large effect on the learning process.
- Fitting limited average severity seems to work better.
  - Brief research shows lower standard errors scaling the ILFs.
- Not scaling may not solve for a more proper scale parameter.

.pull-left[
.tinier[
```{r}
beta  <- 4000
delta <- 0.25
scale <- 50000

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf   = #Pretend these ILFs are your rating factors
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta),
    lev = ilf * scale
  )
```


```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(., shape = params[1], scale = params[2]) / 
    scale
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```
]
]

.pull-right[
```{r echo = FALSE, dpi = 400}

# We repeat the code here to suppress warnings.
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(.,    shape = params[1], scale = params[2]) %>% suppressWarnings()/
    scale
    
  sum((df$ilf - fit)^2) # SSE
  
}

p   <- c(delta = 0.5, beta = 2000) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )

df %>% 
pivot_longer(c(ilf, ilf_fitted)) %>% 
mutate(name = name %>% {if_else(. == "ilf", "Target", "ILF Fitted")}) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) + 
annotate("text", x = 2.5*10^6, y = 0.75, label = TeX(paste0("$\\widehat{\\beta} = ",  round(beta, 2), "$"),  output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 2.5*10^6, y = 0.65, label = TeX(paste0("$\\widehat{\\delta} = ", round(delta, 5), "$"), output = "character"), size = 5, parse = TRUE) + 
annotate("text", x = 4*10^6, y = 0.75, label = TeX(paste0("$\\beta^{\\prime} = ",   round(beta_earlier, 2), "$"),  output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 4*10^6, y = 0.65, label = TeX(paste0("$\\delta^{\\prime} = ",  round(delta_earlier, 5), "$"), output = "character"), size = 5, parse = TRUE) +
annotate("text", x = 3.25*10^6, y = 0.45, label = TeX(paste0("$\\prime$", " parameters using ILF fit"), output = "character"), size = 5, parse = TRUE)
```
]

---

class: inverse, center, middle

# Applications

---

# Interpolation/Extrapolation

Once you have learned the distribution parameters it is trivial to add additional factors to your rating program.

.pull-left[
```{r echo = FALSE, dpi = 400}
#Reset df for simplicity
alpha <- 3
beta  <- 800

interpolate_spots <- c(1500, 2500, 3000, 5000)

df <-
  tibble(
    set   = "Interpolate",
    limit = interpolate_spots,
    ilf   = 
      actuar::levgamma(limit, shape = alpha, scale = beta)/
      actuar::levgamma(2000,  shape = alpha, scale = beta)
  ) %>% 
  rbind(
    tibble(
    set = "Given",
    limit = c(1000, 1:4 * 2000),
    ilf   = 
      actuar::levgamma(limit, shape = alpha, scale = beta)/
      actuar::levgamma(2000,  shape = alpha, scale = beta)
    )
  )

df %>% 
ggplot(aes(x = limit, y = ilf)) +
geom_point(aes(color = set), size = 4) +
# geom_line(aes(color  = set), size = 1) +
geom_line(data  = df %>% filter(set == "Given"), aes(x = limit, y = ilf), color = boost_colors(1), linewidth = 1) +
geom_point(data = df %>% filter(set == "Interpolate"), aes(x = limit, y = ilf), color = boost_colors(4), size = 6) +
theme_bw() +
scale_color_boost(reverse = FALSE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```
]

.pull-right[
```{r echo = FALSE, dpi = 400}
#Reset df for simplicity
alpha <- 4
beta  <- 400

df <-
  tibble(
    set   = "Given",
    limit = c(1.5, 2:5) * 500,
    ilf   = 
      actuar::levgamma(limit, shape = alpha, scale = beta)/
      actuar::levgamma(1000,  shape = alpha, scale = beta)
  ) %>% 
  rbind(
    tibble(
    set = "Extrapolate",
    limit = c(1:3 / 2, 2:8) * 500,
    ilf   = 
      actuar::levgamma(limit, shape = alpha, scale = beta)/
      actuar::levgamma(1000,  shape = alpha, scale = beta)
    )
  )

df %>% 
ggplot(aes(x = limit, y = ilf)) +
geom_point(aes(color = set), size = 4) +
geom_line(aes(color  = set), linewidth = 1) +
geom_point(data = df %>% filter(set == "Given"), aes(x = limit, y = ilf), color = boost_colors(1), size = 4) +
geom_line(data  = df %>% filter(set == "Given"), aes(x = limit, y = ilf), color = boost_colors(1), linewidth = 1) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10)) +
annotate("segment", x = 3400,  xend = 3100,  y = 1.65, yend = 1.42) + 
annotate("text",    x = 3100,  y = 1.36, label = "Be cautious when extrapolating", size = 4) 
```
]

---

# Smoothing 

You may have factors that are not following a smooth pattern. 

```{r echo = FALSE}

df <-
  tribble(
    ~limit, ~ilf,   ~weight,
    50000, 	 0.519, 1,
    100000,	 1.000, 1,
    250000,	 2.136, 1,
    500000,	 3.355, 1,
    750000,	 4.234, 1,
    1000000, 4.950, 1,
    2000000, 5.589, 0,
    3000000, 7.162, 0,
    4000000, 7.877, 0,
    5000000, 8.730, 0
  )

# df <-
#   df %>% filter(limit < 2*10^6 | limit == 5*10^6)

f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(.,    shape = params[1], scale = params[2]) %>% suppressWarnings/
    actuar::levweibull(10^5, shape = params[1], scale = params[2]) %>% suppressWarnings
    
  sum(((df$ilf - fit) * df$weight)^2) # SSE
}

p   <- c(delta = 0.1, beta = 400) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ilf_fitted = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^5,  shape = delta, scale = beta)
  )
```

.pull-left[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(ilf, ilf_fitted)) %>% 
mutate(name = name %>% {if_else(. == "ilf", "Target", "ILF Fitted")}) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```
]

.pull-right[

.tinier[
```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levweibull(.,    shape = params[1], scale = params[2])/
    actuar::levweibull(10^5, shape = params[1], scale = params[2])
    
  sum(((df$ilf - fit) * df$weight)^2) # SSE
}
```
]

```{r echo = FALSE}
df %>% 
mutate(ilf_fitted = round(ilf_fitted, 4)) %>% 
rename_with(~ .x %>% str_replace_all("_", " ") %>% str_to_title %>% str_replace_all("Ilf", "ILF")) %>% 
mutate(Limit = Limit %>% format(big.mark = ",")) %>% 
kbl("html", align = c("l", rep("c", 3))) %>%
kable_styling(latex_options = "striped", font_size = 12) %>% 
column_spec(1:4, width = "25em")
```
]

---

# Trending 

Fit the "outdated" ILFs with this learning method, then apply a trend factor $\tau = 1 + r$ where $r$ is the annual trend.

$$\small{E[\tau \ X; x] = \tau \ E[X; x / \tau]   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \   ILF(\tau \ l) = \frac{E[X; l / \tau]}{E[X; b / \tau]}}$$
.pull-left[
.tinier[
```{r}
# Suppose these are the learned parameters
beta  <- 4000
delta <- 0.25

# Suppose 5 years of historical trend at 5%.
trend <- 1.05 ^ 5 

df <-
  tibble(
    limit = c(0.05 * 1:2, 0.25 * 1:3, 1:5) * 10^6,
    ilf_trend   = 
      actuar::levweibull(limit / trend, shape = delta, scale = beta)/
      actuar::levweibull(10^6  / trend, shape = delta, scale = beta),
    ilf_fitted   = 
      actuar::levweibull(limit, shape = delta, scale = beta)/
      actuar::levweibull(10^6,  shape = delta, scale = beta)
  )
```
]

This method makes it extremely simple to add trend to increased limit factors.

]

.pull-right[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(ilf_fitted, ilf_trend)) %>% 
mutate(name = name %>% {if_else(. == "ilf_fitted", "ILF Fitted", "ILF Trended")}) %>%
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Increased Limit Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```
]

---

# Deductibles

If deductible factors are presented as multiplicative factors you'll need to change the math around: $\big( E[X; \infty] - E[X; d] \big) / E[X; b]$

```{r echo = FALSE}

df <-
  tribble(
  ~deductible, ~factor, ~weight,
  0,	  1.359,	0,
  250,	1.000,	1,
  500,	0.815,	1,
  750,	0.655,  1,
  1000,	0.556,  1
  )

f <- function(params) { 
  
  fit  <- 
    df$deductible %>% 
   {actuar::levweibull(Inf, shape = params[1], scale = params[2]) - 
    actuar::levweibull(.,   shape = params[1], scale = params[2])} %>% suppressWarnings/
    actuar::levweibull(250, shape = params[1], scale = params[2])  %>% suppressWarnings
    
  sum(((df$factor - fit) * df$weight)^2) # SSE
}

p   <- c(delta = 10, beta = 500) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ded_fitted = 
      (actuar::levweibull(Inf, shape = delta, scale = beta) - 
      actuar::levweibull(deductible, shape = delta, scale = beta))/
      actuar::levweibull(250,  shape = delta, scale = beta)
  )
```

.pull-left[
.tinier[
```{r eval = FALSE}

f <- function(params) { 
  
  fit  <- 
    df$deductible %>% 
   {actuar::levweibull(Inf, shape = params[1], scale = params[2]) - 
    actuar::levweibull(.,   shape = params[1], scale = params[2])}/
    actuar::levweibull(250, shape = params[1], scale = params[2])
    
  sum(((df$factor - fit) * df$weight)^2) # SSE
}

p   <- c(delta = 10, beta = 500) # Set initial parameters
opt <- optim(par = p, f) # Optimize

delta <- opt$par[1]
beta  <- opt$par[2]

df <-
  df %>% 
  mutate(
    ded_fitted = 
      (actuar::levweibull(Inf, shape = delta, scale = beta) - 
      actuar::levweibull(deductible, shape = delta, scale = beta))/
      actuar::levweibull(250,  shape = delta, scale = beta)
  )
```
]

Note: in this example there was 0 weight on the zero-dollar deductible. 

<!-- It could have been selected or derived empirically.  In either case there wasn't a good fit including it in the learning process. -->

]

.pull-right[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(factor, ded_fitted)) %>% 
mutate(name = name %>% {if_else(. == "factor", "Target", "Fitted")}) %>%
ggplot(aes(x = deductible, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Deductible") + 
ylab("Deductible Factor") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```
]

---

# Fitting Empirical Data!

```{r echo = FALSE}
alpha <- 3.907288
beta  <- 397.931

# Table 2.2 in https://www.casact.org/sites/default/files/2021-03/8_Bahnemann.pdf
# tibble(
#   limit = c(1, 1 + 0.5* 1:8) * 1000,
#   prob  = limit %>% actuar::ptrgamma(shape1 = alpha, scale = beta, shape2 = 1) %>% {1 - .}, # Pr(X >)
#   las   = limit %>% actuar::levgamma(shape  = alpha, scale = beta)
# )

df <- 
  "https://www.casact.org/sites/default/files/2021-02/02-Bahnemann.pdf" %>% 
  tabulizer::extract_tables(pages = 63, area = list(c(541, 172, 719, 514)), guess = FALSE, output = "data.frame") %>% 
  .[[1]] %>% 
  select(limit = 1, las_sample = 4) %>% 
  mutate(
    across(everything(), ~ .x %>% str_remove(",") %>% as.numeric),
    las_method_of_moments = limit %>% actuar::levgamma(shape = alpha, scale = beta)
  )

f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levgamma(., shape = params[1], scale = params[2]) %>% suppressWarnings()
    
  sum((df$las_sample - fit)^2) # SSE
  
}

p   <- c(alpha = 1, beta = 1000)
opt <- optim(par = p, f)

df <-
  df %>% 
  mutate(
    las_fitted = actuar::levgamma(limit, shape = opt$par[1], scale = opt$par[2])
  )
```

.pull-left[
```{r echo = FALSE, dpi = 400}
df %>% 
pivot_longer(c(las_sample, las_fitted, las_method_of_moments)) %>% 
mutate(name = name %>% str_replace_all("_", " ") %>% str_to_title %>% str_remove_all("Las ")) %>% 
ggplot(aes(x = limit, y = value)) +
geom_line(aes(color  = name), linewidth = 1) +
geom_point(aes(color = name), size = 4) +
theme_bw() +
scale_color_boost(reverse = TRUE) +
theme(
  legend.title    = element_blank(),
  axis.text.x     = element_text(size = 14, hjust = 0.6),
  axis.text.y     = element_text(size = 14),
  axis.title.x    = element_text(size = 16),
  axis.title.y    = element_text(size = 16),
  legend.text     = element_text(size = 16),
  legend.position = "top"
  ) +
xlab("Limit") + 
ylab("Limited Average Severity") +
scale_x_continuous(labels = scales::number_format(big.mark = ",")) + 
scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
theme(axis.text.x = element_text(angle = 10))
```

.tinier[
```{r eval = FALSE}
f <- function(params) { 
  
  fit  <- 
    df$limit %>% 
    actuar::levgamma(., shape = params[1], scale = params[2])
    
  sum((df$las_sample - fit)^2) # SSE
}
```
]
]

.pull-right[

This method produces a better fit than method of moments.

- Method of Moments SSE = `r sum((df$las_sample - df$las_method_of_moments)^2) %>% round(., 0)`
- Learning Method SSE = `r sum((df$las_sample - df$las_fitted)^2) %>% round(., 0)`

```{r echo = FALSE}
df %>% 
mutate(across(everything(), ~ round(.x, 1) %>% format(big.mark = ",", scientific = FALSE))) %>% 
rename(Limit = 1, Sample = 2, MoM = 3, Fitted = 4) %>%  
kbl("html", align = "c") %>%
kable_styling(latex_options = "striped", font_size = 12) %>% 
column_spec(1:4, width = "30em")
```
]

---

class: inverse, center, middle

# Closing Remarks

---

# Final Thoughts

### Other Thoughts

- Fitting in excel (possible but difficult)
- Loss costs in layers instead of rating factors
- ILF $\times$ Deductible
- ILF $\times$ Exposures
- ISO mixed methodology
- Additional research necessary on fitting empirical data

### Learning Process

- Be ready for trial and error.
- Simplify, can get better fits when scaling ILFs or fitting LAS.
  - The scale could be the empirical limited average severity.
- Use weights of zero to remove influence of certain factors.
- Absolute error or log error can be better than sum of squared errors.
- **Be cautious** of how the fitted parameters are used!

---

class: center, middle

# Thank you!

Presentation made with [xaringan](https://github.com/yihui/xaringan)

You can find all code on github: [chuck-fcas/urf2lldp](https://github.com/chuck-fcas/urf2lldp)

Connect with me on [LinkedIn](https://www.linkedin.com/in/charles-lindberg-fcas-35690715)
